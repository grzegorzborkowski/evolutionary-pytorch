{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_iris\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.lin1 = nn.Linear(D_in, H)\n",
    "        self.lin2 = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        return F.softmax(self.lin2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grzegorz/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1472172737121582\n",
      "25 0.9307154417037964\n",
      "50 0.8507824540138245\n",
      "75 0.8206724524497986\n",
      "100 0.8058198094367981\n",
      "125 0.7967326045036316\n",
      "150 0.790061891078949\n",
      "175 0.784167468547821\n",
      "200 0.7775911688804626\n",
      "225 0.7693730592727661\n",
      "250 0.7613940834999084\n",
      "275 0.7545490264892578\n",
      "300 0.748225212097168\n",
      "325 0.7421610951423645\n",
      "350 0.7362945079803467\n",
      "375 0.7306098937988281\n",
      "400 0.7250877618789673\n",
      "425 0.7197133302688599\n",
      "450 0.7144765257835388\n",
      "475 0.7093957662582397\n",
      "prediction accuracy 0.825\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 4, 100, 3\n",
    "torch.manual_seed(7)\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data.data, data.target, test_size=0.8, \n",
    "                                                    random_state = 5)\n",
    "\n",
    "train_X = torch.Tensor(train_X)\n",
    "test_X = torch.Tensor(test_X)\n",
    "train_y = torch.Tensor(train_y).long()\n",
    "test_y = torch.Tensor(test_y).long()\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for t in range(500):\n",
    "  y_pred = model(train_X)\n",
    "  loss = loss_fn(y_pred, train_y)\n",
    "    \n",
    "  if t % 25 == 0:\n",
    "      print(t, loss.item())\n",
    "  \n",
    "  model.zero_grad()\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "      param.data -= learning_rate * param.grad\n",
    "\n",
    "predict_out = model(test_X)\n",
    "_, predict_y = torch.max(predict_out, 1)\n",
    "\n",
    "print ('prediction accuracy', accuracy_score(test_y, predict_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
